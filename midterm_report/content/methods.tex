\section{Methods}
\label{sec:methods}
Solving this problem will consist of two parts: 
\begin{itemize}
  \item A traffic simulation implementing the IDM and MOBIL,
  \item a programm analysing the data generated by the simulation.
\end{itemize}
Those two components will be nearly independent from each other.

\subsection{Traffic Simulation}
\label{sec:traffic_simulation}
The programm which simulates traffic and generates the data will be written in C++ for performance reasons. To get
meaningfull results, a few hundred cars will be simulated and as soon as you implement lane changes $\mathcal{O}(n^2)$
parts start to apear to find new following cars (with $n$ being the amount of cars). The multi lane road is implemented
as a class, which stores the cars in a vector from the standart template library. As a time integration method currently
Euler`s method is used, which might be changes to a 4th order Runge Kutta in the future. The data will be stores in
comma-seperated value (CSV) files. Altough csv files are slower than binary data files, they have the advantage to be
human readable for debugging purposes. Even with a slow output and $\mathcal{O}(n^2)$ parts in the algorithms,
a few hundred cars can be simulated in realtime on a laptop. 

The only part where analysis and simulation overlap is when it comes to macroscopic observables: Eventhough the data 
analysis programm explained later can calculate these observables from the generated microscopic data, parts of it will
be implemented inside the simulation. This not only makes the calculation more stable since no information gets lost or
cut of before the calculation, but also make the data sharing between the two programms smaller and less prone to error.

\subsection{Data Analysis}
\label{sec:data_analysis}
For the data analysis we will use python. The data can be easily imported with pandas and visualized in matplotlib.
Since the simulation can store every location of a car at any time, even video animations can be made with, making the
simulating part more transparent. Since we are transforming the large data set of many microscopic observables into a
few macroscopic, only simple analysis tools like curve fits and similar numerical methods can be used. For this part
numpy and scipy are two highly capable libraries.

